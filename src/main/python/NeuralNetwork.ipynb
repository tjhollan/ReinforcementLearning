{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of tensorflow activation function string aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `softmax`\n",
    "- `relu`\n",
    "- `elu`\n",
    "- `tanh`\n",
    "- `sigmoid`\n",
    "- `hard_sigmoid`\n",
    "- `linear`\n",
    "- `softplus`\n",
    "- `softsign`\n",
    "- `selu` \n",
    "- `gelu` \n",
    "- `relu6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting neuralnetworks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile neuralnetworks.py\n",
    "from tensorflow.keras import models, layers, optimizers, backend as K\n",
    "import numpy as np\n",
    "\n",
    "##########################\n",
    "#  Neural Network Class  #\n",
    "##########################\n",
    "\n",
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh', drop=False):\n",
    "        inputs = layers.Input(name=\"input\", shape=(n_inputs,))\n",
    "        hidden_layers = self._create_hidden_layers(n_hiddens_per_layer, inputs, activation_function, drop)\n",
    "        outputs = layers.Dense(name=\"output\", units=n_outputs, activation='linear')(hidden_layers)\n",
    "        self.model = models.Model(inputs=inputs, outputs=outputs, name=\"DeepNN\")\n",
    "\n",
    "    def _create_hidden_layers(self, n_hiddens_per_layer, input_layer, activation_function, drop):\n",
    "        count = 0\n",
    "        previous_layer = input_layer\n",
    "        for size_of_hidden_layer in n_hiddens_per_layer:\n",
    "            count += 1\n",
    "            layer_name = f\"hidden{count:03}\"\n",
    "            previous_layer = layers.Dense(name=layer_name, units=size_of_hidden_layer, activation=activation_function)(previous_layer)\n",
    "            if drop:\n",
    "                drop_name = f\"drop{count:03}\"\n",
    "                previous_layer = layers.Dropout(name=drop_name, rate=0.1)(previous_layer)\n",
    "        return previous_layer\n",
    "\n",
    "    def R_squared(self, y, y_hat):\n",
    "        ss_res =  K.sum(K.square(y - y_hat)) \n",
    "        ss_tot = K.sum(K.square(y - K.mean(y))) \n",
    "        return ( 1 - ss_res/(ss_tot + K.epsilon()) )\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate=0.001, method='adam', verbose=False):\n",
    "        if method == 'adam':\n",
    "            optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            optimizer = method\n",
    "        self.model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=[self.R_squared])\n",
    "        verbose_number = 0 if not verbose else 1\n",
    "        self.model.fit(x=X, y=T, epochs=n_epochs, batch_size=None, shuffle=True, verbose=verbose_number, validation_split=0.0)\n",
    "        return self\n",
    "        \n",
    "    def use(self, X):\n",
    "        return self.model(X, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralnetworks as nn\n",
    "import numpy as np\n",
    "\n",
    "def test_neuralnetwork(verbose=False):\n",
    "    np.random.seed(42)\n",
    "        \n",
    "    n_samples = 10000\n",
    "    X = np.linspace(0, 10, n_samples).reshape((-1, 1))\n",
    "    T = X ** 2\n",
    "\n",
    "    n_samples, n_inputs = X.shape \n",
    "    n_outputs = T.shape[1]\n",
    "\n",
    "    n_hiddens = [100, 100]\n",
    "    net = nn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, activation_function='relu')\n",
    "    net.train(X, T, 50, 0.01, verbose=verbose)\n",
    "    Y = net.use(X)\n",
    "\n",
    "    def rmse(Y, T):\n",
    "        return np.sqrt(np.mean((T - Y)**2))\n",
    "\n",
    "    print(f'RMSE {rmse(Y, T):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 0.313\n"
     ]
    }
   ],
   "source": [
    "test_neuralnetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 1s 885us/step - loss: 3.0790 - R_squared: 0.9289\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.6392 - R_squared: 0.9987\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 0s 788us/step - loss: 0.6929 - R_squared: 0.9984\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 0s 807us/step - loss: 0.4952 - R_squared: 0.9993\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 0s 804us/step - loss: 0.4262 - R_squared: 0.9995\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 0s 791us/step - loss: 0.5547 - R_squared: 0.9990\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 0s 804us/step - loss: 0.4707 - R_squared: 0.9993\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 0s 826us/step - loss: 0.4756 - R_squared: 0.9993\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 0s 787us/step - loss: 0.4935 - R_squared: 0.9993\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 0s 905us/step - loss: 0.5060 - R_squared: 0.9992\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 0s 922us/step - loss: 0.4999 - R_squared: 0.9992\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 0s 926us/step - loss: 0.4629 - R_squared: 0.9993\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 0s 848us/step - loss: 0.5595 - R_squared: 0.9990\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 0s 849us/step - loss: 0.5365 - R_squared: 0.9991\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 0s 889us/step - loss: 0.4716 - R_squared: 0.9993\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 0s 836us/step - loss: 0.4731 - R_squared: 0.9993\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 0s 838us/step - loss: 0.4510 - R_squared: 0.9992\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 0s 840us/step - loss: 0.3344 - R_squared: 0.9997\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 0s 909us/step - loss: 0.4039 - R_squared: 0.9995\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 0s 804us/step - loss: 0.3946 - R_squared: 0.9995\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 0s 788us/step - loss: 0.4809 - R_squared: 0.9992\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 0s 813us/step - loss: 0.4345 - R_squared: 0.9994\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 0s 843us/step - loss: 0.3359 - R_squared: 0.9996\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 0s 834us/step - loss: 0.3987 - R_squared: 0.9995\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 0s 807us/step - loss: 0.4555 - R_squared: 0.9992\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 0s 793us/step - loss: 0.3812 - R_squared: 0.9996\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 0s 851us/step - loss: 0.3887 - R_squared: 0.9995\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 0s 895us/step - loss: 0.4300 - R_squared: 0.9994\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 0s 828us/step - loss: 0.3829 - R_squared: 0.9995\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 0s 908us/step - loss: 0.3397 - R_squared: 0.9997\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 0s 817us/step - loss: 0.4926 - R_squared: 0.9992\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 0s 847us/step - loss: 0.3900 - R_squared: 0.9995\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 0s 820us/step - loss: 0.4735 - R_squared: 0.9993\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 0s 810us/step - loss: 0.3587 - R_squared: 0.9996\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 0s 804us/step - loss: 0.3542 - R_squared: 0.9996\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 0s 820us/step - loss: 0.3823 - R_squared: 0.9995\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 0s 796us/step - loss: 0.4290 - R_squared: 0.9993\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 0s 803us/step - loss: 0.2997 - R_squared: 0.9997\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 0s 821us/step - loss: 0.3625 - R_squared: 0.9996\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 0s 805us/step - loss: 0.4488 - R_squared: 0.9993\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 0s 800us/step - loss: 0.3188 - R_squared: 0.9996\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 0s 811us/step - loss: 0.4398 - R_squared: 0.9993\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 0s 805us/step - loss: 0.4834 - R_squared: 0.9992\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 0s 800us/step - loss: 0.2926 - R_squared: 0.9997\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 0s 859us/step - loss: 0.3081 - R_squared: 0.9997\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 0s 871us/step - loss: 0.3542 - R_squared: 0.9996\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 0s 809us/step - loss: 0.3505 - R_squared: 0.9996\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 0s 813us/step - loss: 0.4634 - R_squared: 0.9993\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 0s 806us/step - loss: 0.2580 - R_squared: 0.9998\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 0s 798us/step - loss: 0.2714 - R_squared: 0.9998\n",
      "RMSE 0.294\n"
     ]
    }
   ],
   "source": [
    "test_neuralnetwork(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e284ee3255a07ad8bf76694974743c4c81cb57e7c969474d752d949b11d721e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
