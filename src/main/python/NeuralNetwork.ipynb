{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of tensorflow activation function string aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `softmax`\n",
    "- `relu`\n",
    "- `elu`\n",
    "- `tanh`\n",
    "- `sigmoid`\n",
    "- `hard_sigmoid`\n",
    "- `linear`\n",
    "- `softplus`\n",
    "- `softsign`\n",
    "- `selu` \n",
    "- `gelu` \n",
    "- `relu6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers, backend as K\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
    "        ### layer input\n",
    "        inputs = layers.Input(name=\"input\", shape=(n_inputs,))\n",
    "        if len(n_hiddens_per_layer) < 1:\n",
    "            outputs = layers.Dense(name=\"output\", units=n_outputs, activation=activation_function)\n",
    "        else:\n",
    "            hidden_layers = self._create_hidden_layers(n_hiddens_per_layer, inputs, activation_function)\n",
    "            outputs = layers.Dense(name=\"output\", units=n_outputs, activation=activation_function)(hidden_layers)\n",
    "        self.model = models.Model(inputs=inputs, outputs=outputs, name=\"DeepNN\")\n",
    "\n",
    "    def _create_hidden_layers(self, n_hiddens_per_layer, input_layer, activation_function):\n",
    "        count = 1\n",
    "        previous_layer = input_layer\n",
    "        for size_of_hidden_layer in n_hiddens_per_layer:\n",
    "            layer_name = f\"hidden{count:03}\"\n",
    "            previous_layer = layers.Dense(name=layer_name, units=size_of_hidden_layer, activation=activation_function)(previous_layer)\n",
    "            previous_layer = layers.Dropout(name=\"drop2\", rate=0.2)(previous_layer)\n",
    "        return previous_layer\n",
    "\n",
    "    def R2(y, y_hat):\n",
    "        ss_res =  K.sum(K.square(y - y_hat)) \n",
    "        ss_tot = K.sum(K.square(y - K.mean(y))) \n",
    "        return ( 1 - ss_res/(ss_tot + K.epsilon()) )\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate=0.001, method='adam', verbose=False, visuals=False):\n",
    "        if method == 'adam':\n",
    "            optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            optimizer = method\n",
    "        self.model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=[self.R2])\n",
    "        self.training = self.model.fit(x=X, y=T, batch_size=32, epochs=n_epochs, shuffle=True, verbose=verbose, validation_split=0.3)\n",
    "        \n",
    "    def use(self, X):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 10)]              0         \n",
      "                                                                 \n",
      " h1 (Dense)                  (None, 6)                 66        \n",
      "                                                                 \n",
      " drop1 (Dropout)             (None, 6)                 0         \n",
      "                                                                 \n",
      " h2 (Dense)                  (None, 3)                 21        \n",
      "                                                                 \n",
      " drop2 (Dropout)             (None, 3)                 0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91\n",
      "Trainable params: 91\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DeepNN\n",
    "### layer input\n",
    "inputs = layers.Input(name=\"input\", shape=(n_features,))\n",
    "### hidden layer 1\n",
    "h1 = layers.Dense(name=\"h1\", units=int(round((n_features+1)/2)), activation='relu')(inputs)\n",
    "h1 = layers.Dropout(name=\"drop1\", rate=0.2)(h1)\n",
    "### hidden layer 2\n",
    "h2 = layers.Dense(name=\"h2\", units=int(round((n_features+1)/4)), activation='relu')(h1)\n",
    "h2 = layers.Dropout(name=\"drop2\", rate=0.2)(h2)\n",
    "### layer output\n",
    "outputs = layers.Dense(name=\"output\", units=1, activation='sigmoid')(h2)\n",
    "model = models.Model(inputs=inputs, outputs=outputs, name=\"DeepNN\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "def Recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def Precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def F1(y_true, y_pred):\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# compile the neural network\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "              metrics=['accuracy',F1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "def R2(y, y_hat):\n",
    "    ss_res =  K.sum(K.square(y - y_hat)) \n",
    "    ss_tot = K.sum(K.square(y - K.mean(y))) \n",
    "    return ( 1 - ss_res/(ss_tot + K.epsilon()) )\n",
    "\n",
    "# compile the neural network\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error', \n",
    "              metrics=[R2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validation\n",
    "training = model.fit(x=X, y=y, batch_size=32, epochs=100, shuffle=True, verbose=0, validation_split=0.3)\n",
    "\n",
    "# plot\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,3))\n",
    "       \n",
    "## training    \n",
    "ax[0].set(title=\"Training\")    \n",
    "ax11 = ax[0].twinx()    \n",
    "ax[0].plot(training.history['loss'], color='black')    ax[0].set_xlabel('Epochs')    \n",
    "ax[0].set_ylabel('Loss', color='black')    \n",
    "for metric in metrics:        \n",
    "    ax11.plot(training.history[metric], label=metric)    ax11.set_ylabel(\"Score\", color='steelblue')    \n",
    "ax11.legend()\n",
    "        \n",
    "## validation    \n",
    "ax[1].set(title=\"Validation\")    \n",
    "ax22 = ax[1].twinx()    \n",
    "ax[1].plot(training.history['val_loss'], color='black')    ax[1].set_xlabel('Epochs')    \n",
    "ax[1].set_ylabel('Loss', color='black')    \n",
    "for metric in metrics:          \n",
    "    ax22.plot(training.history['val_'+metric], label=metric)    ax22.set_ylabel(\"Score\", color=\"steelblue\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e284ee3255a07ad8bf76694974743c4c81cb57e7c969474d752d949b11d721e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
